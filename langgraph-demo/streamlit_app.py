"""Streamlitå‰ç«¯åº”ç”¨ï¼ˆæç®€ç‰ˆï¼‰"""

import streamlit as st
import os
import uuid
from pathlib import Path

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from agent.data_processor.file_manager import FileManager
from agent.data_processor.parser import DocumentParser  # å…¼å®¹æ€§å¯¼å…¥
from agent.data_processor.chunker import chunk_documents
from agent.vector_store.factory import VectorStoreFactory
from agent.vector_store.base import DocumentChunk
from agent.utils.embedding import generate_embeddings
from agent.rag.graph import get_rag_graph
from agent.rag.state import GraphRAGState
from agent.config import config
from langchain_core.messages import HumanMessage


# é¡µé¢é…ç½®
st.set_page_config(
    page_title="é‡‘èå›¾RAGæ™ºèƒ½é—®ç­”",
    page_icon="ğŸ’¼",
    layout="wide"
)

# åˆå§‹åŒ–session state
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = []
if "user_id" not in st.session_state:
    st.session_state.user_id = str(uuid.uuid4())


def process_uploaded_files(uploaded_files):
    """å¤„ç†å·²é€‰æ‹©çš„ä¸Šä¼ æ–‡ä»¶ï¼Œå°†å…¶å†™å…¥å‘é‡æ•°æ®åº“"""
    if not uploaded_files:
        st.warning("è¯·å…ˆé€‰æ‹©è¦ä¸Šä¼ çš„æ–‡æ¡£ã€‚")
        return

    vector_store = VectorStoreFactory.create_vector_store()
    vector_store.initialize()

    progress_bar = st.progress(0)
    status_text = st.empty()

    all_chunks = []

    for idx, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name}")

        # ä¿å­˜æ–‡ä»¶
        upload_dir = Path(config.upload_dir)
        upload_dir.mkdir(exist_ok=True)
        file_path = upload_dir / uploaded_file.name

        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        try:
            # ä½¿ç”¨FileManagerè§£ææ–‡æ¡£ï¼ˆæ”¯æŒæ›´å¤šæ ¼å¼ï¼Œæ›´å¥½çš„ä¸­æ–‡æ”¯æŒï¼‰
            if not FileManager.is_supported(str(file_path)):
                st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {uploaded_file.name}")
                continue
            
            content = FileManager.parse_file(str(file_path))

            # åˆ†å—
            doc_id = str(uuid.uuid4())
            chunks = chunk_documents(
                content=content,
                doc_id=doc_id,
                user_id=st.session_state.user_id,
                doc_type=uploaded_file.name.split(".")[-1],
            )

            # ç”ŸæˆåµŒå…¥å‘é‡
            chunk_texts = [chunk["content"] for chunk in chunks]
            embeddings = generate_embeddings(chunk_texts)

            # åˆ›å»ºDocumentChunkå¯¹è±¡
            # ä½¿ç”¨UUIDä½œä¸ºIDï¼Œç¡®ä¿Weaviateå…¼å®¹æ€§
            document_chunks = []
            for i, chunk in enumerate(chunks):
                # ç”Ÿæˆå”¯ä¸€çš„UUIDä½œä¸ºID
                chunk_uuid = str(uuid.uuid4())
                document_chunks.append(
                    DocumentChunk(
                        id=chunk_uuid,
                        content=chunk["content"],
                        metadata={
                            **chunk["metadata"],
                            "original_chunk_id": f"{doc_id}_{chunk['metadata']['chunk_index']}",  # ä¿ç•™åŸå§‹IDåœ¨metadataä¸­
                        },
                        embedding=embeddings[i],
                    )
                )

            # æ’å…¥å‘é‡æ•°æ®åº“
            vector_store.add_documents(document_chunks)
            all_chunks.extend(chunks)

            progress_bar.progress((idx + 1) / len(uploaded_files))

        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")

    status_text.text("å¤„ç†å®Œæˆï¼")
    st.success(f"æˆåŠŸå¤„ç† {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œå…± {len(all_chunks)} ä¸ªæ–‡æ¡£å—")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for uploaded_file in uploaded_files:
        file_path = upload_dir / uploaded_file.name
        if file_path.exists():
            file_path.unlink()


def chat_interface():
    """å¯¹è¯ç•Œé¢ï¼ˆä¸Šä¸‹å¸ƒå±€ + ä¸Šä¼ /å‘é€æŒ‰é’®ï¼‰"""
    st.markdown("### ğŸ’¬ ä¸Šä¸‹æ–‡å¯¹è¯")

    # å¯¹è¯å†å²åŒºåŸŸ
    chat_container = st.container()
    with chat_container:
        if not st.session_state.conversation_history:
            st.info("è¿˜æ²¡æœ‰å¯¹è¯å†…å®¹ï¼Œå…ˆä¸Šä¼ æ–‡æ¡£æˆ–ç›´æ¥å¼€å§‹æé—®å§ã€‚")
        for i, (question, answer) in enumerate(st.session_state.conversation_history):
            st.markdown(f"**é—®ï¼š** {question}")
            st.markdown(f"**ç­”ï¼š** {answer}")
            st.divider()

    st.markdown("---")

    # ä¸‹åŠéƒ¨åˆ†ï¼šè¾“å…¥ + ä¸Šä¼  + æŒ‰é’®
    with st.container():
        user_input = st.text_area("åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜ï¼š", key="user_query", height=100)

        # ä¸Šä¼ æ§ä»¶ + æŒ‰é’®ä¸€è¡Œ
        col_upload, col_send = st.columns([1, 1])

        with col_upload:
            uploaded_files = st.file_uploader(
                "é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡æ¡£ï¼ˆPDF/DOCX/DOC/PPT/Excel/TXTï¼‰",
                type=["pdf", "docx", "doc", "pptx", "ppt", "xlsx", "xls", "txt"],
                accept_multiple_files=True,
                key="chat_uploader",
            )
            upload_clicked = st.button("ğŸ“ ä¸Šä¼ å¹¶å…¥åº“", use_container_width=True)

        with col_send:
            send_clicked = st.button("ğŸ“¨ å‘é€å¯¹è¯", type="primary", use_container_width=True)

    # å¤„ç†ä¸Šä¼ æŒ‰é’®é€»è¾‘
    if upload_clicked:
        with st.spinner("æ­£åœ¨ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£..."):
            process_uploaded_files(uploaded_files)

    # å¤„ç†å‘é€å¯¹è¯æŒ‰é’®é€»è¾‘
    if send_clicked and user_input:
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            try:
                # è°ƒç”¨RAGå›¾
                rag_graph = get_rag_graph()

                initial_state = GraphRAGState(
                    messages=[HumanMessage(content=user_input)],
                    query=user_input,
                    user_id=st.session_state.user_id,
                    retrieved_chunks=[],
                    retrieval_scores=[],
                    reflection_result=None,
                    needs_iteration=False,
                    iteration_count=0,
                    refined_query=None,
                    final_answer=None,
                    metadata={},
                )

                result = rag_graph.invoke(initial_state)
                answer = result.get("final_answer", "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚")

                # ä¿å­˜åˆ°å†å²
                st.session_state.conversation_history.append((user_input, answer))

                # åˆ·æ–°é¡µé¢ä»¥å±•ç¤ºæ–°å¯¹è¯
                st.rerun()

            except Exception as e:
                st.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")


st.markdown("## ğŸ’¼ é‡‘èå›¾RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
st.caption(f"ä¼šè¯ç”¨æˆ· IDï¼š`{st.session_state.user_id[:8]}...`")

# ä¸»å†…å®¹åŒºï¼šä¸Šä¸‹æ–‡å¯¹è¯ç•Œé¢
chat_interface()
